import pandas as pd
import numpy as np
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from itertools import combinations
import missingno
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')
ds = pd.read_csv("/kaggle/input/iris-flower-dataset/IRIS.csv")
print(ds)
ds.head()
ds.species.value_counts()
ds.info()
ds.isnull().sum().sort_values(ascending = False) 
ds.shape
ds.describe()
ds.nunique()
ds.species.unique()
ds.sepal_length.unique()
ds.sepal_width.unique()
ds.petal_length.unique()
ds.petal_width.unique()
figg = px.histogram(data_frame=ds, x='species', title='total no of species', color='species')
figg.show()
def Histogram(column):
    global ds
    fig = go.Figure()
    fig.add_trace(go.Histogram(x=ds[column], marker_color='skyblue', marker_line_color='black', marker_line_width=1.5))

    fig.update_layout(title=f'Histogram of {column}',
                      xaxis_title=column,
                      yaxis_title='Frequency',
                      template='plotly_dark')
fig.show()
Histogram('sepal_length')
Histogram('petal_length')
Histogram('sepal_width')
Histogram('petal_width')
plt.subplot(2,2,1)
sns.boxplot(ds['sepal_length'],color='red')

plt.tight_layout()
plt.show()

plt.subplot(2,2,2)
sns.boxplot(ds['sepal_width'],color='red')

plt.tight_layout()
plt.show()plt.subplot(2,2,3)
sns.boxplot(ds['petal_length'],color='purple')

plt.tight_layout()
plt.show()
plt.subplot(2,2,4)
sns.boxplot(ds['petal_width'],color='purple')

plt.tight_layout()
plt.show()
fig1 = px.histogram(ds, x="species", color="sepal_length")  
fig1.show()  
fig2 = px.histogram(ds, x="species", color="sepal_width")  
fig2.show()  

fig3 = px.histogram(ds, x="species", color="petal_width")  
fig3.show()  
fig4= px.histogram(ds, x="species", color="petal_length")  
fig4.show()  
def Scatterplot(x, y, c=None):
    global ds
    
    plt.figure(figsize=(15, 6))
    for species in ds['species'].unique():
        plt.scatter(ds[x][ds['species'] == species], ds[y][ds['species'] == species], label=species, edgecolor="k", alpha=0.7)
    plt.xticks(rotation=0)
    
    plt.title("Scatter plot X:{} / Y:{}".format(x, y))
    plt.xlabel(x)
    plt.ylabel(y)
    plt.legend()
    plt.show()
comb = combinations(["species", "petal_leng
fig4= px.histogram(ds, x="species", color="petal_length")  
fig4.show()  def Scatterplot(x, y, c=None):
    global ds
    
    plt.figure(figsize=(15, 6))
    for species in ds['species'].unique():
        plt.scatter(ds[x][ds['species'] == species], ds[y][ds['species'] == species], label=species, edgecolor="k", alpha=0.7)
    plt.xticks(rotation=0)
    
    plt.title("Scatter plot X:{} / Y:{}".format(x, y))
    plt.xlabel(x)
    plt.ylabel(y)
    plt.legend()
    plt.show()
comb = combinations(["species
comb = combinations(["species", "petal_length", "petal_width", "sepal_length", "sepal_width"], 2)
comb_list = [list(i) for i in comb]
for col in comb_list:
    Scatterplot(col[0], col[1])
ds.head()
ds.species.unique()
species_encoding = ds.groupby('species').agg({'sepal_length': 'mean'}).to_dict()
ds['species_encoded'] = round(ds['species'].map(species_encoding['sepal_length']), 1)
ds.species_encoded.unique()
missingno.bar(ds, color="dodgerblue", sort="ascending", figsize=(6,3), fontsize=12);
sns.heatmap(ds.corr(method='spearman'),annot=True,cmap="BrBG");
X = ds.drop(['species_encoded'], axis=1)
y = ds['species_encoded']
X.head()
lr = LinearRegression()
lr.fit(X_train,y_train)
lr_pred_train = lr.predict(X_train)
lr_pred_test = lr.predict(X_test)
print('Mean Squared Error (Linear Regression - Test):',   mean_squared_error(y_test, lr_pred_test))
print('Mean Absolute Error  (Linear Regression - Test): ',mean_absolute_error(y_test, lr_pred_test))
print('R2 score (Linear Regression - Train): ',           r2_score(y_train,lr_pred_train))
print('R2 Score (Linear Regression - Test):',             r2_score(y_test,lr_pred_test))
rfg = RandomForestRegressor()
rfg.fit(X_train,y_train)
rf_pred_train = rfg.predict(X_train)
rf_pred_test = rfg.predict(X_test)
print('Mean Squared Error (Random Forest - Test):',mean_squared_error(y_test, rf_pred_test))
print('Mean Absolute Error  (Random Forest - Test): ',mean_absolute_error(y_test, rf_pred_test))
print('R2 score (Random Forest - Train): ',r2_score(y_train,rf_pred_train))
print('R2 Score (Random Forest - Test):',r2_score(y_test,rf_pred_test))
xgboost = XGBRegressor(objective='reg:squarederror', random_state=42)
xgboost.fit(X_train, y_train)
y_pred_xgboost_train = xgboost.predict(X_train)
y_pred_xgboost_test = xgboost.predict(X_test)
print('Mean Squared Error (xgboost - Test):', mean_squared_error(y_test, y_pred_xgboost_test))
print('Mean Absolute Error  (xgboost - Test): ',mean_absolute_error(y_test, y_pred_xgboost_test))
print('R2 score (xgboost - Train): ', r2_score(y_train, y_pred_xgboost_train))
print('R2 Score (xgboost - Test):', r2_score(y_test,y_pred_xgboost_test))
dt = DecisionTreeRegressor(max_depth=10,random_state=0)
dt.fit(X_train,y_train)
y_pred = dt.predict(X_train)
y_pred_test = dt.predict(X_test)
print('Mean Squared Error (Decision Tree - Test):',mean_squared_error(y_test, y_pred_test))
print('Mean Absolute Error  (Decision Tree - Test): ',mean_absolute_error(y_test, y_pred_test))
print('R2 score (Decision Tree - Train): ',r2_score(y_train,y_pred))
print('R2 Score (Decision Tree - Test):',r2_score(y_test,y_pred_test))
mae_lr_train = mean_absolute_error(y_train, lr_pred_train)
mae_lr_test = mean_absolute_error(y_test, lr_pred_test)
mse_lr_train = mean_squared_error(y_train, lr_pred_train)
mse_lr_test = mean_squared_error(y_test, lr_pred_test
mae_rf_train = mean_absolute_error(y_train, rf_pred_train)
mae_rf_test = mean_absolute_error(y_test, rf_pred_test)
mse_rf_train = mean_squared_error(y_train, rf_pred_train)
mse_rf_test = mean_squared_error(y_test, rf_pred_test)

# Evaluate Decision Tree Regressor
mae_dt_train = mean_absolute_error(y_train, y_pred)
mae_dt_test = mean_absolute_error(y_test, y_pred_test)
mse_dt_train = mean_squared_error(y_train, y_pred)
mse_dt_test = mean_squared_error(y_test, y_pred_test)

# Evaluate xgboost Regressor
mae_xg_train = mean_absolute_error(y_train, y_pred_xgboost_train)
mae_xg_test = mean_absolute_error(y_test, y_pred_xgboost_test)
mse_xg_train = mean_squared_error(y_train, y_pred_xgboost_train)
mse_xg_test = mean_squared_error(y_test, y_pred_xgboost_test)
# Create DataFrames for plotting
metrics_df = pd.DataFrame({
    'Model': ['Linear Regression', 'Random Forest Regressor', 'decision tree','xgboost Regressor'],
    'MAE Train': [mae_lr_train, mae_rf_train,mae_dt_train,mae_xg_train],
    'MAE Test': [mae_lr_test, mae_rf_test, mae_dt_test,mae_xg_test],
    'MSE Train': [mse_lr_train, mse_rf_train, mse_dt_train,mse_xg_train],
    'MSE Test': [mse_lr_test, mse_rf_test, mse_dt_test,mse_xg_test]
})
# Plotting MAE
fig_mae = px.bar(metrics_df, x='Model', y=['MAE Train', 'MAE Test'],
                 title='Mean Absolute Error (MAE)',
                 labels={'value': 'MAE', 'variable': 'Dataset'},
                 color_discrete_map={'MAE Train': 'blue', 'MAE Test': 'red'})
fig_mae.update_layout(barmode='group')

# Show the plots
fig_mae.show()
fig_mse = px.bar(metrics_df, x='Model', y=['MSE Train', 'MSE Test'],
                 title='Mean Squared Error (MSE)',
                 labels={'value': 'MSE', 'variable': 'Dataset'},
                 color_discrete_map={'MSE Train': 'blue', 'MSE Test': 'red'})
fig_mse.update_layout(barmode='group')

fig_mse.show()
scatter_color = 'skyblue'
plot_width = 800
plot_height = 275
fig_train = px.scatter(x=y_train, y=y_pred_xgboost_train, labels={'x': 'Actual Ratings', 'y': 'Predicted Ratings'},
                       title='Evaluation of Training xgboost Regressor Model', trendline='ols',
                       color_discrete_sequence=[scatter_color])
fig_train.update_layout(showlegend=True, width=plot_width, height=plot_height)

# Scatter plot for Testing Set
fig_test = px.scatter(x=y_test, y=y_pred_xgboost_test, labels={'x': 'Actual Ratings', 'y': 'Predicted Ratings'},
                      title='Evaluation of Testing xgboost Regressor Model', trendline='ols',
                      color_discrete_sequence=[scatter_color])
fig_test.update_layout(showlegend=True, width=plot_width, height=plot_height)
fig_train.show()
fig_test.show()
scatter_color = 'skyblue'
plot_width = 800
plot_height = 275
fig_train = px.scatter(x=y_train, y=rf_pred_train, labels={'x': 'Actual Ratings', 'y': 'Predicted Ratings'},
                       title='Evaluation of Training xgboost Model', trendline='ols',
                       color_discrete_sequence=[scatter_color])
fig_train.update_layout(showlegend=True, width=plot_width, height=plot_height)
fig_test = px.scatter(x=y_test, y=rf_pred_test, labels={'x': 'Actual Ratings', 'y': 'Predicted Ratings'},
                      title='Evaluation of Testing xgboost Model', trendline='ols',
                      color_discrete_sequence=[scatter_color])
fig_test.update_layout(showlegend=True, width=plot_width, height=plot_height)

# Show the plots
fig_train.show()
fig_test.show()
prediction_xgboost = pd.DataFrame({'actual_species': y_test,
                        'predicted_species': y_pred_xgboost_test.ravel(),
                        'residual': y_test - y_pred_xgboost_test}
                      )
prediction_xgboost.head(10)
	actual_species	predicted_species	residual
55	5.9	5.900010	-0.000010
22	5.0	5.000022	-0.000022
26	5.0	5.000022	-0.000022
56	5.9	6.004890	-0.104890
134	6.6	6.516637	0.083363
131	6.6	6.600221	-0.000221
23	5.0	5.000022	-0.000022
34	5.0	5.000331	-0.000331
85	5.9	5.892816	0.007184
21	5.0	5.000022	-0.000022

